{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/agustinlopez/Repositories/TrialGPT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/agustinlopez/Repositories/TrialGPT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.28.0\n"
     ]
    }
   ],
   "source": [
    "import ipykernel\n",
    "print(ipykernel.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agustinlopez/anaconda3/envs/trialGPT/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from nltk import word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def preprocess_entry(entry):\n",
    "    title = entry.get(\"title\", \"\")\n",
    "    text = entry.get(\"text\", \"\")\n",
    "\n",
    "    if not isinstance(title, str):\n",
    "        title = str(title)\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    return title, text\n",
    "\n",
    "\n",
    "def process_bm25_chunk(chunk, tokenized_corpus, corpus_nctids):\n",
    "    for entry in chunk:\n",
    "        corpus_nctids.append(entry[\"_id\"])\n",
    "        title, _ = preprocess_entry(entry)\n",
    "        tokens = word_tokenize(title.lower()) * 3\n",
    "\n",
    "        for disease in entry[\"metadata\"][\"diseases_list\"]:\n",
    "            tokens += word_tokenize(disease.lower()) * 2\n",
    "\n",
    "        tokens += word_tokenize(entry[\"text\"].lower())\n",
    "        tokenized_corpus.append(tokens)\n",
    "\n",
    "\n",
    "def process_medcpt_chunk(chunk, embeds, corpus_nctids, tokenizer, model):\n",
    "    for entry in chunk:\n",
    "        corpus_nctids.append(entry[\"_id\"])\n",
    "        title, text = preprocess_entry(entry)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded = tokenizer(\n",
    "                [[title, text]],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors='pt',\n",
    "                max_length=512,\n",
    "            )\n",
    "            embed = model(**encoded).last_hidden_state[:, 0, :]\n",
    "            embeds.append(embed[0].cpu().numpy())\n",
    "\n",
    "\n",
    "def read_first_n_entries(file_path, n=20):\n",
    "    entries = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= n:\n",
    "                break\n",
    "            entries.append(json.loads(line))\n",
    "    return entries\n",
    "\n",
    "\n",
    "def get_bm25_corpus_index(corpus):\n",
    "    corpus_path = os.path.join(f\"trialgpt_retrieval/bm25_corpus_{corpus}.json\")\n",
    "\n",
    "    if os.path.exists(corpus_path):\n",
    "        corpus_data = json.load(open(corpus_path))\n",
    "        tokenized_corpus = corpus_data[\"tokenized_corpus\"]\n",
    "        corpus_nctids = corpus_data[\"corpus_nctids\"]\n",
    "    else:\n",
    "        tokenized_corpus = []\n",
    "        corpus_nctids = []\n",
    "\n",
    "        # Leer solo los primeros 20 registros de corpus.jsonl\n",
    "        first_20_entries = read_first_n_entries(f\"dataset/{corpus}/corpus.jsonl\", n=20)\n",
    "        process_bm25_chunk(first_20_entries, tokenized_corpus, corpus_nctids)\n",
    "\n",
    "        corpus_data = {\n",
    "            \"tokenized_corpus\": tokenized_corpus,\n",
    "            \"corpus_nctids\": corpus_nctids,\n",
    "        }\n",
    "\n",
    "        with open(corpus_path, \"w\") as f:\n",
    "            json.dump(corpus_data, f, indent=4)\n",
    "\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25, corpus_nctids\n",
    "\n",
    "\n",
    "def get_medcpt_corpus_index(corpus):\n",
    "    corpus_path = f\"trialgpt_retrieval/{corpus}_embeds.npy\"\n",
    "    nctids_path = f\"trialgpt_retrieval/{corpus}_nctids.json\"\n",
    "\n",
    "    if os.path.exists(corpus_path):\n",
    "        embeds = np.load(corpus_path)\n",
    "        corpus_nctids = json.load(open(nctids_path))\n",
    "    else:\n",
    "        print(f\"Building MedCPT corpus index for {corpus}\")\n",
    "        embeds = []\n",
    "        corpus_nctids = []\n",
    "\n",
    "        print(\"Loading MedCPT model and tokenizer...\")\n",
    "        model = AutoModel.from_pretrained(\"ncbi/MedCPT-Article-Encoder\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Article-Encoder\")\n",
    "        print(\"MedCPT model and tokenizer loaded successfully.\")\n",
    "\n",
    "        # Leer solo los primeros 20 registros de corpus.jsonl\n",
    "        first_20_entries = read_first_n_entries(f\"dataset/{corpus}/corpus.jsonl\", n=20)\n",
    "        process_medcpt_chunk(first_20_entries, embeds, corpus_nctids, tokenizer, model)\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        np.save(corpus_path, embeds)\n",
    "        with open(nctids_path, \"w\") as f:\n",
    "            json.dump(corpus_nctids, f, indent=4)\n",
    "\n",
    "    index = faiss.IndexFlatIP(768)\n",
    "    index.add(embeds)\n",
    "    return index, corpus_nctids\n",
    "\n",
    "\n",
    "def preprocess_corpus(corpus_texts, max_length=200):\n",
    "    preprocessed_texts = []\n",
    "    for text in corpus_texts:\n",
    "        # Divide el texto en trozos más pequeños si es demasiado largo\n",
    "        if len(text) > max_length:\n",
    "            for i in range(0, len(text), max_length):\n",
    "                preprocessed_texts.append(text[i:i + max_length])\n",
    "        else:\n",
    "            preprocessed_texts.append(text)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:00<00:00, 17695.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MedCPT corpus index for metabase\n",
      "Loading MedCPT model and tokenizer...\n",
      "MedCPT model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = \"metabase\"\n",
    "q_type = \"gpt-4-turbo\"\n",
    "k = 20\n",
    "bm25_wt = 1\n",
    "medcpt_wt = 1\n",
    "N = 2000\n",
    "\n",
    "# Load the qrels\n",
    "_, _, qrels = GenericDataLoader(data_folder=f\"dataset/{corpus}/\").load(split=\"test\")\n",
    "\n",
    "# Load all types of queries\n",
    "id2queries = json.load(open(f\"dataset/{corpus}/id2queries.json\"))\n",
    "\n",
    "# Preprocess the corpus texts\n",
    "preprocessed_corpus = preprocess_corpus([query[q_type] if q_type in query else \"\" for query in id2queries.values()])\n",
    "\n",
    "# Create a new dictionary with preprocessed texts\n",
    "id2queries_preprocessed = {key: preprocessed_corpus[i] for i, key in enumerate(id2queries.keys())}\n",
    "\n",
    "# Load the indices\n",
    "bm25, bm25_nctids = get_bm25_corpus_index(corpus)\n",
    "medcpt, medcpt_nctids = get_medcpt_corpus_index(corpus)\n",
    "\n",
    "# Load the query encoder for MedCPT\n",
    "model = AutoModel.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
    "\n",
    "# Conduct the searches, saving top 1k\n",
    "output_path = f\"results/qid2nctids_results_{q_type}_{corpus}_k{k}_bm25wt{bm25_wt}_medcptwt{medcpt_wt}_N{N}.json\"\n",
    "\n",
    "qid2nctids = {}\n",
    "recalls = []\n",
    "\n",
    "with open(f\"dataset/{corpus}/queries.jsonl\", \"r\") as f:\n",
    "    for line in tqdm.tqdm(f.readlines()):\n",
    "        entry = json.loads(line)\n",
    "        query = entry[\"text\"]\n",
    "        qid = entry[\"_id\"]\n",
    "\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "\n",
    "        truth_sum = sum(qrels[qid].values())\n",
    "\n",
    "        # get the keyword list\n",
    "        if q_type in [\"raw\", \"human_summary\"]:\n",
    "            conditions = [id2queries_preprocessed[qid]]\n",
    "        elif \"turbo\" in q_type:\n",
    "            conditions = id2queries[qid][q_type][\"conditions\"]\n",
    "        elif \"Clinician\" in q_type:\n",
    "            conditions = id2queries[qid].get(q_type, [])\n",
    "\n",
    "        if len(conditions) == 0:\n",
    "            nctid2score = {}\n",
    "        else:\n",
    "            # a list of nctid lists for the bm25 retriever\n",
    "            bm25_condition_top_nctids = []\n",
    "\n",
    "            for condition in conditions:\n",
    "                tokens = word_tokenize(condition.lower())\n",
    "                top_nctids = bm25.get_top_n(tokens, bm25_nctids, n=N)\n",
    "                bm25_condition_top_nctids.append(top_nctids)\n",
    "\n",
    "            # doing MedCPT retrieval\n",
    "            with torch.no_grad():\n",
    "                encoded = tokenizer(\n",
    "                    conditions,\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    return_tensors='pt',\n",
    "                    max_length=256,\n",
    "                )\n",
    "\n",
    "                # encode the queries (use the [CLS] last hidden states as the representations)\n",
    "                embeds = model(**encoded).last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "                # search the Faiss index\n",
    "                scores, inds = medcpt.search(embeds, k=N)\n",
    "\n",
    "            medcpt_condition_top_nctids = []\n",
    "            for ind_list in inds:\n",
    "                top_nctids = [medcpt_nctids[ind] for ind in ind_list]\n",
    "                medcpt_condition_top_nctids.append(top_nctids)\n",
    "\n",
    "            nctid2score = {}\n",
    "\n",
    "            for condition_idx, (bm25_top_nctids, medcpt_top_nctids) in enumerate(\n",
    "                    zip(bm25_condition_top_nctids, medcpt_condition_top_nctids)):\n",
    "\n",
    "                if bm25_wt > 0:\n",
    "                    for rank, nctid in enumerate(bm25_top_nctids):\n",
    "                        if nctid not in nctid2score:\n",
    "                            nctid2score[nctid] = 0\n",
    "\n",
    "                        nctid2score[nctid] += (1 / (rank + k)) * (1 / (condition_idx + 1))\n",
    "\n",
    "                if medcpt_wt > 0:\n",
    "                    for rank, nctid in enumerate(medcpt_top_nctids):\n",
    "                        if nctid not in nctid2score:\n",
    "                            nctid2score[nctid] = 0\n",
    "\n",
    "                        nctid2score[nctid] += (1 / (rank + k)) * (1 / (condition_idx + 1))\n",
    "\n",
    "        nctid2score = sorted(nctid2score.items(), key=lambda x: -x[1])\n",
    "        top_nctids = [nctid for nctid, _ in nctid2score[:N]]\n",
    "        qid2nctids[qid] = top_nctids\n",
    "\n",
    "        actual_sum = sum([qrels[qid].get(nctid, 0) for nctid in top_nctids])\n",
    "        recalls.append(actual_sum / truth_sum)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(qid2nctids, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trialGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
